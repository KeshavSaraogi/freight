{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "output_dir = \"/Users/keshavsaraogi/Desktop/freight\"\n",
    "data_dir = \"/path/to/dataset/folder\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_traffic_data(street_prefix):\n",
    "    \"\"\"\n",
    "    Combines all traffic CSV files for a given street prefix.\n",
    "    :param street_prefix: Prefix of the street files (e.g., \"And\", \"Bel\", \"Bxl\").\n",
    "    :return: Combined DataFrame with traffic data.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(data_dir) if f.startswith(street_prefix) and f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        print(f\"Loading {file_path}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "traffic_data_and = load_traffic_data(\"And\")\n",
    "\n",
    "def save_to_parquet(dataframe, output_file):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a Parquet file.\n",
    "    :param dataframe: pandas DataFrame to save\n",
    "    :param output_file: Output file path for the Parquet file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe.to_parquet(output_file, engine=\"pyarrow\", index=False)\n",
    "        print(f\"Data successfully saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Parquet: {e}\")\n",
    "\n",
    "output_file = os.path.join(output_dir, \"traffic_data_and.parquet\")\n",
    "save_to_parquet(traffic_data_and, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geo_data_files = [\n",
    "    \"/Users/keshavsaraogi/Desktop/freight/Anderlecht_streets.json\",\n",
    "    \"/Users/keshavsaraogi/Desktop/freight/Belgium_streets.json,\n",
    "    \"/Users/keshavsaraogi/Desktop/freight/Bruxelles_streets.json\",\n",
    "    \"/Users/keshavsaraogi/Desktop/freight/bruxelles.json\"\n",
    "]\n",
    "\n",
    "# Function to load and combine geospatial data\n",
    "def combine_geospatial_data(file_list):\n",
    "    \"\"\"\n",
    "    Combines multiple GeoJSON files into a single GeoDataFrame.\n",
    "    :param file_list: List of file paths to GeoJSON files.\n",
    "    :return: Combined GeoDataFrame.\n",
    "    \"\"\"\n",
    "    gdfs = []\n",
    "    for file in file_list:\n",
    "        print(f\"Loading {file}...\")\n",
    "        gdf = gpd.read_file(file)\n",
    "        gdfs.append(gdf)\n",
    "    \n",
    "    # Concatenate all GeoDataFrames\n",
    "    combined_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "    \n",
    "    # Ensure a consistent CRS (Coordinate Reference System)\n",
    "    if combined_gdf.crs is None:\n",
    "        combined_gdf.set_crs(\"EPSG:4326\", inplace=True)  # Default to WGS84 if CRS is missing\n",
    "    else:\n",
    "        combined_gdf = combined_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    return combined_gdf\n",
    "\n",
    "# Combine geospatial data\n",
    "combined_geo_data = combine_geospatial_data(geo_data_files)\n",
    "\n",
    "# Save combined geospatial data to a GeoJSON file\n",
    "output_file = \"/path/to/combined_geospatial_data.geojson\"\n",
    "combined_geo_data.to_file(output_file, driver=\"GeoJSON\")\n",
    "print(f\"Combined geospatial data saved to {output_file}\")\n",
    "\n",
    "# Optional: Plot the combined data\n",
    "combined_geo_data.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Info:\")\n",
    "print(combined_gdf.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(combined_gdf.isnull().sum())\n",
    "\n",
    "print(\"\\nCoordinate Reference System (CRS):\")\n",
    "print(combined_gdf.crs)\n",
    "\n",
    "print(\"\\nInvalid Geometries:\")\n",
    "print(combined_gdf[~combined_gdf.is_valid])  \n",
    "print(\"\\nEmpty Geometries:\")\n",
    "print(combined_gdf[combined_gdf.is_empty])  \n",
    "\n",
    "print(\"\\nDuplicate Entries:\")\n",
    "duplicates = combined_gdf.duplicated(subset=None, keep=False)\n",
    "print(combined_gdf[duplicates])\n",
    "\n",
    "if \"geometry\" not in combined_gdf.columns:\n",
    "    print(\"\\nError: No 'geometry' column found!\")\n",
    "elif combined_gdf['geometry'].isnull().any():\n",
    "    print(\"\\nError: Null geometries found!\")\n",
    "else:\n",
    "    print(\"\\nGeometry column is valid.\")\n",
    "\n",
    "combined_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the structure of the dataset\n",
    "print(\"Info:\")\n",
    "print(traffic_data.info())\n",
    "\n",
    "# 2. Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(traffic_data.isnull().sum())\n",
    "\n",
    "# 3. Check for duplicate rows\n",
    "print(\"\\nDuplicate Entries:\")\n",
    "duplicates = traffic_data.duplicated(keep=False)\n",
    "print(traffic_data[duplicates])\n",
    "\n",
    "# 4. Check for invalid or outlier values in numerical columns\n",
    "numerical_columns = traffic_data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "print(\"\\nOutlier Statistics (5th and 95th percentiles):\")\n",
    "for col in numerical_columns:\n",
    "    lower = traffic_data[col].quantile(0.05)\n",
    "    upper = traffic_data[col].quantile(0.95)\n",
    "    print(f\"{col}: [{lower}, {upper}]\")\n",
    "\n",
    "# 5. Validate timestamps (convert to datetime and check for issues)\n",
    "if \"timestamp\" in traffic_data.columns:\n",
    "    try:\n",
    "        traffic_data[\"timestamp\"] = pd.to_datetime(traffic_data[\"timestamp\"], errors=\"coerce\")\n",
    "        print(\"\\nInvalid Timestamps:\")\n",
    "        print(traffic_data[traffic_data[\"timestamp\"].isnull()])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting timestamps: {e}\")\n",
    "else:\n",
    "    print(\"\\nWarning: No 'timestamp' column found!\")\n",
    "\n",
    "# 6. Check for negative or zero values in critical columns (e.g., traffic_volume)\n",
    "if \"traffic_volume\" in traffic_data.columns:\n",
    "    invalid_traffic = traffic_data[traffic_data[\"traffic_volume\"] <= 0]\n",
    "    print(\"\\nInvalid Traffic Volume:\")\n",
    "    print(invalid_traffic)\n",
    "else:\n",
    "    print(\"\\nWarning: No 'traffic_volume' column found!\")\n",
    "\n",
    "# 7. Visualize distributions of key metrics (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in [\"traffic_volume\", \"average_speed\"]:  # Adjust based on your data\n",
    "    if col in traffic_data.columns:\n",
    "        traffic_data[col].plot(kind=\"hist\", bins=50, title=f\"Distribution of {col}\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
